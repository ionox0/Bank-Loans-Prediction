{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loans Prediction - Predicting successful Loan Subscriptions\n",
    "\n",
    "### Ian Johnson and Daniel First\n",
    "\n",
    "A banking institution ran a direct marketing campaign based on phone calls. Often, more than one contact to the same client was required, in order to assess if the product (bank term deposit) would be subscribed or not. Your task is to predict whether someone will subscribe to the term deposit or not based on the given information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0 - Import Libraries, Load Data\n",
    "\n",
    "This is the basic step where you can load the data and create train and test sets for internal validation as per your convinience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "# Features\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, Imputer, MaxAbsScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, SelectPercentile, VarianceThreshold\n",
    "from sklearn.feature_selection import RFE, f_classif, mutual_info_classif\n",
    "\n",
    "# Models\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, LassoCV, RidgeCV, ElasticNet, ElasticNetCV\n",
    "from sklearn.discriminant_analysis import  LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Testing\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "pd.options.display.max_columns = 999\n",
    "delim = '\\n\\n' + '*'*30\n",
    "\n",
    "holdout = pd.read_csv('data/holdout.csv')\n",
    "data = pd.read_csv('data/data.csv')\n",
    "\n",
    "# Map labels to boolean values\n",
    "data['subscribed'] = data['subscribed'].map(lambda x: 0 if x == 'no' else 1)\n",
    "\n",
    "# Drop credit_defult as no one in holdout set has 'yes' value\n",
    "\n",
    "# ********\n",
    "subscribed = data['subscribed']\n",
    "data_withtarget=data\n",
    "# ***********\n",
    "\n",
    "holdout_ids = holdout['ID']\n",
    "data = data.drop([\"subscribed\", \"duration\", \"credit_default\"], axis=1)\n",
    "holdout = holdout.drop([\"ID\", \"duration\", \"credit_default\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>education</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>campaign</th>\n",
       "      <th>prev_days</th>\n",
       "      <th>prev_contacts</th>\n",
       "      <th>prev_outcomes</th>\n",
       "      <th>emp_var_rate</th>\n",
       "      <th>cons_price_idx</th>\n",
       "      <th>cons_conf_idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr_employed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.0</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.9y</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>apr</td>\n",
       "      <td>mon</td>\n",
       "      <td>2.0</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.695118</td>\n",
       "      <td>92.698705</td>\n",
       "      <td>-46.727552</td>\n",
       "      <td>1.345160</td>\n",
       "      <td>5097.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.0</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>may</td>\n",
       "      <td>wed</td>\n",
       "      <td>2.0</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.767159</td>\n",
       "      <td>92.914878</td>\n",
       "      <td>-46.313088</td>\n",
       "      <td>1.314499</td>\n",
       "      <td>5100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56.0</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.9y</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>fri</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-0.100365</td>\n",
       "      <td>93.423076</td>\n",
       "      <td>-41.904559</td>\n",
       "      <td>4.003471</td>\n",
       "      <td>5193.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.0</td>\n",
       "      <td>retired</td>\n",
       "      <td>divorced</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>may</td>\n",
       "      <td>wed</td>\n",
       "      <td>4.0</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.771314</td>\n",
       "      <td>93.672814</td>\n",
       "      <td>-46.045500</td>\n",
       "      <td>1.261668</td>\n",
       "      <td>5100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34.0</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>jul</td>\n",
       "      <td>thu</td>\n",
       "      <td>8.0</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.458103</td>\n",
       "      <td>94.296285</td>\n",
       "      <td>-42.455877</td>\n",
       "      <td>5.152077</td>\n",
       "      <td>5233.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age           job marital_status          education housing loan  \\\n",
       "0  41.0   blue-collar        married           basic.9y     yes   no   \n",
       "1  46.0  entrepreneur        married            unknown      no   no   \n",
       "2  56.0    unemployed        married           basic.9y     yes  yes   \n",
       "3  89.0       retired       divorced           basic.4y     yes   no   \n",
       "4  34.0  entrepreneur        married  university.degree     yes   no   \n",
       "\n",
       "    contact month day_of_week  campaign  prev_days  prev_contacts  \\\n",
       "0  cellular   apr         mon       2.0        999              0   \n",
       "1  cellular   may         wed       2.0        999              0   \n",
       "2  cellular   nov         fri       1.0        999              0   \n",
       "3  cellular   may         wed       4.0        999              0   \n",
       "4  cellular   jul         thu       8.0        999              0   \n",
       "\n",
       "  prev_outcomes  emp_var_rate  cons_price_idx  cons_conf_idx  euribor3m  \\\n",
       "0   nonexistent     -1.695118       92.698705     -46.727552   1.345160   \n",
       "1   nonexistent     -1.767159       92.914878     -46.313088   1.314499   \n",
       "2   nonexistent     -0.100365       93.423076     -41.904559   4.003471   \n",
       "3   nonexistent     -1.771314       93.672814     -46.045500   1.261668   \n",
       "4   nonexistent      1.458103       94.296285     -42.455877   5.152077   \n",
       "\n",
       "   nr_employed  \n",
       "0       5097.0  \n",
       "1       5100.0  \n",
       "2       5193.0  \n",
       "3       5100.0  \n",
       "4       5233.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Exploration and Preparation\n",
    "\n",
    "In this step, we expect you to look into the data and try to understand it before modeling. This understanding may lead to some basic data preparation steps which are common across the two model sets required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categorical_df=data.select_dtypes(include=['object'])\n",
    "categorical_variables=categorical_df.columns\n",
    "\n",
    "frames = [categorical_df, subscribed]\n",
    "categ_and_target = pd.concat(frames,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating new features\n",
    "#### Boolean variable indicating whether participant falls into subcategories that exhibit higher proportion of successful loan subscriptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobs:\n",
      "{'management': 0, 'retired': 0, 'self-employed': 0, 'unknown': 0, 'unemployed': 0, 'housemaid': 0, 'admin.': 0, 'entrepreneur': 0, 'services': 0, 'student': 0, 'technician': 0, 'blue-collar': 0}\n",
      "Months:\n",
      "{'mar': 0, 'aug': 0, 'sep': 0, 'apr': 0, 'jun': 0, 'jul': 0, 'may': 0, 'nov': 0, 'dec': 0, 'oct': 0}\n",
      "Education:\n",
      "{'basic.9y': 0, 'illiterate': 0, 'basic.4y': 0, 'unknown': 0, 'basic.6y': 0, 'high.school': 0, 'professional.course': 0, 'university.degree': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Jobs:\")\n",
    "dict_percentage_job={}\n",
    "job_categories=categ_and_target.job.unique()\n",
    "\n",
    "for each_job in job_categories:\n",
    "    just_that_job_df=categ_and_target[categ_and_target.job==each_job]\n",
    "    percentage=len(just_that_job_df[just_that_job_df.subscribed==1])/len(just_that_job_df)\n",
    "    dict_percentage_job[each_job]=percentage\n",
    "print(dict_percentage_job)\n",
    "\n",
    "print(\"Months:\")\n",
    "dict_percentage_month={}\n",
    "month_categories=categ_and_target.month.unique()\n",
    "\n",
    "for each_month in month_categories:\n",
    "    just_that_month_df=categ_and_target[categ_and_target.month==each_month]\n",
    "    percentage=len(just_that_month_df[just_that_month_df.subscribed==1])/len(just_that_month_df)\n",
    "    dict_percentage_month[each_month]=percentage\n",
    "print(dict_percentage_month)\n",
    "\n",
    "\n",
    "print(\"Education:\")\n",
    "dict_percentage_edu={}\n",
    "edu_categories=categ_and_target.education.unique()\n",
    "\n",
    "for each_edu in edu_categories:\n",
    "    just_that_edu_df=categ_and_target[categ_and_target.education==each_edu]\n",
    "    percentage=len(just_that_edu_df[just_that_edu_df.subscribed==1])/len(just_that_edu_df)\n",
    "    dict_percentage_edu[each_edu]=percentage\n",
    "print(dict_percentage_edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def month_function(month):\n",
    "    if month==\"dec\" or month==\"mar\" or month==\"oct\" or month==\"sep\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def job_function(job):\n",
    "    if job==\"student\" or job == \"retired\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Note: 0s and 1s switched\n",
    "def education_function(y):\n",
    "    if y==\"basic.9y\" or y==\"basic.4y\" or y==\"basic.6y\" or y==\"high.school\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_counts(x,dict):\n",
    "    return dict[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def new_feats(data):\n",
    "    data_withbool=data\n",
    "    \n",
    "    data_withbool[\"monthbool\"]=data.month.apply(lambda x:month_function(x))\n",
    "    data_withbool[\"jobbool\"]=data.job.apply(lambda x:job_function(x))\n",
    "    data_withbool[\"educationbool\"]=data.education.apply(lambda x:education_function(x))\n",
    "    \n",
    "    data_withbool_withcounts=data_withbool\n",
    "    data_withbool_withcounts[\"educationcounts\"]=data.education.apply(lambda x:get_counts(x,dict_percentage_edu))\n",
    "    data_withbool_withcounts[\"jobcounts\"]=data.job.apply(lambda x:get_counts(x,dict_percentage_job))\n",
    "    data_withbool_withcounts[\"monthcounts\"]=data.month.apply(lambda x:get_counts(x,dict_percentage_month))\n",
    "    \n",
    "    dict_edu_2={}\n",
    "    dict_edu_2[\"basic.4y\"]=1\n",
    "    dict_edu_2[\"basic.6y\"]=2\n",
    "    dict_edu_2[\"basic.9y\"]=3\n",
    "    dict_edu_2[\"high.school\"]=4\n",
    "    dict_edu_2[\"professional.course\"]=5\n",
    "    dict_edu_2[\"university.degree\"]=6\n",
    "    dict_edu_2[\"unknown\"]=7\n",
    "    dict_edu_2[\"illiterate\"]=8\n",
    "\n",
    "    data_withbool_withcounts[\"edu_linear\"]=data.education.apply(lambda x:get_counts(x,dict_edu_2))\n",
    "    \n",
    "    return data_withbool_withcounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features consisting of the logs of each of the continuous columns from the datasest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Log features\n",
    "def log_feats(data):\n",
    "    for c in data.select_dtypes(exclude=['object']).columns:\n",
    "        data[c + '__log'] = data[c].apply(lambda x: np.log(abs(x) + 0.001))\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features consisting of the mean of each of the continuous columns, for each subcategory of the categorical columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_mean_cont_per_cat_group(df):\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    continuous_cols = df.select_dtypes(exclude=['object']).columns\n",
    "    \n",
    "    means = {}\n",
    "    \n",
    "    for cat in categorical_cols:\n",
    "        for u in df[cat].unique():\n",
    "            for con in continuous_cols:\n",
    "                subset = df[cat] == u\n",
    "                mean_value = df.loc[subset, con].mean()\n",
    "                \n",
    "                if not cat in means:\n",
    "                    means[cat] = {}\n",
    "                else:\n",
    "                    means[cat][u] = mean_value\n",
    "                \n",
    "    return means\n",
    "\n",
    "\n",
    "def transform_mean_cont_per_cat_group(df, means):\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    continuous_cols = df.select_dtypes(exclude=['object']).columns\n",
    "    \n",
    "    for cat in categorical_cols:\n",
    "        for u in df[cat].unique():\n",
    "            for con in continuous_cols:\n",
    "                \n",
    "                subset = df[cat] == u\n",
    "                mean_value = means[cat][u]\n",
    "                df.loc[subset, cat + '__' + con + '__mean'] = mean_value\n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply new feature creation functions to the data and holdout data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32950, 18)\n",
      "(8238, 18)\n",
      "(32950, 99)\n",
      "(8238, 99)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(holdout.shape)\n",
    "means = fit_mean_cont_per_cat_group(data)\n",
    "data_new_feats_1 = transform_mean_cont_per_cat_group(data, means)\n",
    "holdout_new_feats_1 = transform_mean_cont_per_cat_group(holdout, means)\n",
    "print(data_new_feats_1.shape)\n",
    "print(holdout_new_feats_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32950, 106)\n",
      "(8238, 106)\n"
     ]
    }
   ],
   "source": [
    "data_new_feats_2 = new_feats(data)\n",
    "holdout_new_feats_2 = new_feats(holdout)\n",
    "print(data_new_feats_2.shape)\n",
    "print(holdout_new_feats_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32950, 203)\n",
      "(8238, 203)\n"
     ]
    }
   ],
   "source": [
    "data_new_feats_3 = log_feats(data_new_feats_2)\n",
    "holdout_new_feats_3 = log_feats(holdout_new_feats_2)\n",
    "print(data_new_feats_3.shape)\n",
    "print(holdout_new_feats_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32950, 244)\n",
      "(8238, 244)\n"
     ]
    }
   ],
   "source": [
    "data_dummies = pd.get_dummies(data_new_feats_3)\n",
    "holdout_dummies = pd.get_dummies(holdout_new_feats_3)\n",
    "print(data_dummies.shape)\n",
    "print(holdout_dummies.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split of our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24712, 244)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data_dummies, subscribed, random_state=42)# stratify=subscribed, random_state=42)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility grid search function to assess subsequent models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grid_search_metrics(pipe, param_grid):\n",
    "    grid = GridSearchCV(pipe, param_grid=param_grid, scoring='roc_auc')\n",
    "    grid.fit(x_train, y_train)\n",
    "    print(\"Best GS score: {}\".format(grid.best_score_))\n",
    "    print(\"Best params: {}\".format(grid.best_params_))\n",
    "    score = grid.score(x_test, y_test)\n",
    "    print(\"Best test score: {}\".format(score))\n",
    "    print(\"Overfitting amount: {}\".format(grid.best_score_ - score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Model Set 1\n",
    "\n",
    "In this step, we perform the following steps relevant to exploring our initial options for modeling:\n",
    "\n",
    "* validation\n",
    "* feature selection\n",
    "* final model selection\n",
    "\n",
    "We limit ourselves to linear models for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - Best score: 0.7608"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best GS score: 0.759512868405\n",
      "Best params: {'model__C': 0.5}\n",
      "Best test score: 0.760780878347\n",
      "Overfitting amount: -0.00126800994195\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"variance\", VarianceThreshold()),\n",
    "    (\"selection_1\", SelectKBest(score_func=f_classif)),\n",
    "    (\"polys\", PolynomialFeatures()),\n",
    "    (\"scaling\", MinMaxScaler()),\n",
    "    (\"selection_2\", SelectKBest(score_func=f_classif)),\n",
    "    (\"model\", LogisticRegression())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'model__C': [.5], #[1, 0.5, .1, .001]\n",
    "}\n",
    "\n",
    "grid_search_metrics(pipe, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Model Set 2\n",
    "\n",
    "\n",
    "In this step, we perform the following steps relevant to exploring our initial options for modeling:\n",
    "\n",
    "* validation\n",
    "* feature selection\n",
    "* final model selection\n",
    "\n",
    "We explore tree-based methods in this model set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier - Best score: 0.7828"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best GS score: 0.788095295181\n",
      "Best params: {'model__max_features': 14, 'selection_1__k': 55, 'model__criterion': 'entropy', 'model__max_depth': 3, 'selection_2__k': 600, 'model__min_samples_leaf': 10, 'model__min_samples_split': 2, 'model__bootstrap': True}\n",
      "Best test score: 0.782781588094\n",
      "Overfitting amount: 0.00531370708682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [   0  163 1594] are constant.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"variance\", VarianceThreshold()),\n",
    "    (\"selection_1\", SelectKBest(score_func=f_classif)),\n",
    "    (\"polys\", PolynomialFeatures()),\n",
    "    (\"scaling\", StandardScaler()),\n",
    "    (\"selection_2\", SelectKBest(score_func=f_classif)),\n",
    "    (\"model\", RandomForestClassifier())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'selection_1__k': [55], #[45, 50, 55],\n",
    "    'selection_2__k': [600], #[600, 700, 800],\n",
    "    \n",
    "    \"model__max_depth\": [3], #[3, None],\n",
    "    \"model__max_features\": [14], #[13, 14, 15],\n",
    "    \"model__min_samples_split\": [2], #[2, 3, 4],\n",
    "    \"model__min_samples_leaf\": [10], #[7, 10, 15],\n",
    "    \"model__bootstrap\": [True], #[True, False],\n",
    "    \"model__criterion\": ['entropy'], #[\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "grid_search_metrics(pipe, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier - Best score: 0.7846"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best GS score: 0.787467734305\n",
      "Best params: {'model__max_features': 'sqrt', 'selection_1__k': 50, 'model__learning_rate': 0.1, 'model__max_depth': 6, 'selection_2__k': 700, 'model__min_samples_leaf': 40, 'model__min_samples_split': 600, 'model__random_state': 10, 'model__subsample': 0.8}\n",
      "Best test score: 0.784558637634\n",
      "Overfitting amount: 0.0029090966718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [   0  148 1324] are constant.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"variance\", VarianceThreshold()),\n",
    "    (\"selection_1\", SelectKBest(score_func=f_classif)),\n",
    "    (\"polys\", PolynomialFeatures()),\n",
    "    (\"scaling\", StandardScaler()),\n",
    "    (\"selection_2\", SelectKBest(score_func=f_classif)),\n",
    "    (\"model\", GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'selection_1__k': [50],\n",
    "    'selection_2__k': [700],\n",
    "    \n",
    "    'model__max_features': ['sqrt'],\n",
    "    'model__learning_rate': [.1],\n",
    "    'model__max_depth': [6], # [5, 6, 7],\n",
    "    'model__min_samples_leaf': [40], # [39, 40, 41],\n",
    "    'model__min_samples_split': [600], #[590, 600, 610],\n",
    "    'model__random_state': [10],\n",
    "    'model__subsample': [0.8], #[0.75, 0.8, 0.85],\n",
    "}\n",
    "\n",
    "grid_search_metrics(pipe, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron Classifier - Best score: 0.7873"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best GS score: 0.776304905481\n",
      "Best params: {'model__hidden_layer_sizes': (10, 10, 10), 'model__alpha': 0.0001, 'selection_2__k': 700, 'selection_1__k': 50}\n",
      "Best test score: 0.787252008921\n",
      "Overfitting amount: -0.0109471034393\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"variance\", VarianceThreshold()),\n",
    "    (\"selection_1\", SelectKBest(score_func=f_classif)),\n",
    "    (\"polys\", PolynomialFeatures()),\n",
    "    (\"scaling\", StandardScaler()),\n",
    "    (\"selection_2\", SelectKBest(score_func=f_classif)),\n",
    "    (\"model\", MLPClassifier())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'selection_1__k': [50],\n",
    "    'selection_2__k': [700],\n",
    "    \n",
    "    'model__alpha': [0.0001], #, 0.001],\n",
    "    'model__hidden_layer_sizes': [(10, 10, 10)], #[(5, 5, 5), (10, 10, 10), (20, 20, 20), (30, 30, 30), (40, 40, 40)]\n",
    "}\n",
    "\n",
    "grid_search_metrics(pipe, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Ensemble\n",
    "\n",
    "In this step, we ensemble the tuned models from the previous steps:\n",
    "- LogisticRegression\n",
    "- RandomForest\n",
    "- GradientBoosting\n",
    "- MultiLayerPerceptron\n",
    "\n",
    "Our final choice for this section, after multiple runs with different pipeline parameters, achieved a test score of:\n",
    "\n",
    "### 0.7898\n",
    "\n",
    "This is an improvement of about\n",
    "\n",
    "### 0.002\n",
    "\n",
    "over our individual tuned models on their own. We will later explore stacking with this ensemble model to make further improvements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# svc = LinearSVC(C=0.5)\n",
    "# clf1 = CalibratedClassifierCV(svc, method='sigmoid')\n",
    "\n",
    "lr = LogisticRegression(C=0.5)\n",
    "clf1 = CalibratedClassifierCV(lr, method='sigmoid')\n",
    "\n",
    "rf1 = RandomForestClassifier(\n",
    "    max_features = 10,\n",
    "    criterion = 'gini',\n",
    "    max_depth = 3,\n",
    "    min_samples_leaf = 10,\n",
    "    min_samples_split = 3,\n",
    "    bootstrap = True,\n",
    ")\n",
    "clf2 = CalibratedClassifierCV(rf1, method='sigmoid')\n",
    "\n",
    "gb1 = GradientBoostingClassifier(\n",
    "    max_features='sqrt',\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    min_samples_leaf=40,\n",
    "    min_samples_split=600,\n",
    "    random_state=10,\n",
    "    subsample=0.8\n",
    ")\n",
    "clf3 = CalibratedClassifierCV(gb1, method='sigmoid')\n",
    "\n",
    "clf4 = MLPClassifier(alpha = 0.0001, hidden_layer_sizes = (20, 20, 20))\n",
    "\n",
    "\n",
    "eclf1 = VotingClassifier(voting='soft', estimators=[\n",
    "    ('one', clf1),\n",
    "    ('two', clf2),\n",
    "    ('three', clf3),\n",
    "    ('four', clf4),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"variance\", VarianceThreshold()),\n",
    "    (\"selection_1\", SelectKBest(score_func=f_classif)),\n",
    "    (\"polys\", PolynomialFeatures()),\n",
    "    (\"scaling\", StandardScaler()),\n",
    "    (\"selection_2\", SelectKBest(score_func=f_classif)),\n",
    "    (\"model\", eclf1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best GS score: 0.794581376016\n",
      "Best params: {'model__voting': 'soft', 'selection_2__k': 900, 'selection_1__k': 70}\n",
      "Best test score: 0.78979773368\n",
      "Overfitting amount: 0.0047836423357\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'selection_1__k': [70],\n",
    "    'selection_2__k': [900],\n",
    "    'model__voting': ['soft'],\n",
    "}\n",
    "\n",
    "grid_search_metrics(pipe, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking\n",
    "\n",
    "Here we explore the use of [stacking](https://en.wikipedia.org/wiki/Ensemble_learning#Stacking) to improve our test score. \n",
    "\n",
    "We apply a Logistic Regression Classifier on top of the predicted probabilities from the previous Voting Classifier ensemble to achieve an AUC_ROC score of:\n",
    "\n",
    "### 0.7930\n",
    "\n",
    "Which is an improvement of \n",
    "\n",
    "### 0.0032\n",
    "\n",
    "over the previous ensemble alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"variance\", VarianceThreshold()),\n",
    "    (\"selection_1\", SelectKBest(score_func=f_classif, k=50)),\n",
    "    (\"polys\", PolynomialFeatures()),\n",
    "    (\"scaling\", StandardScaler()),\n",
    "    (\"selection_2\", SelectKBest(score_func=f_classif, k=700)),\n",
    "    (\"model\", eclf1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.90931531239883456, 0.89621267297887841)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "reshaper = FunctionTransformer(lambda X_: np.rollaxis(X_, 1).reshape(-1, 8)[:, 1::2], validate=False)\n",
    "stacking = Pipeline([\n",
    "    ('first_stage', eclf1),\n",
    "    ('reshaper', reshaper),\n",
    "    ('second_stage', LogisticRegression(C=100))\n",
    "])\n",
    "\n",
    "stacking.fit(x_train, y_train)\n",
    "train_score = stacking.score(x_train, y_train)\n",
    "test_score = stacking.score(x_test, y_test)\n",
    "test_preds = stacking.predict_proba(x_test)\n",
    "print(train_score, test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79299000091617677"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_score = aoc_auc_score(y_test, test_preds[:,1])\n",
    "assert final_score > 0.79"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holdout Predictions with best model\n",
    "\n",
    "Finally we make predictions on the holdout set with our best stacked ensemble for submission to the Kaggle competition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.90887018452573654, 0.89633406166545282)\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"variance\", VarianceThreshold()),\n",
    "    (\"selection_1\", SelectKBest(score_func=f_classif, k=50)),\n",
    "    (\"polys\", PolynomialFeatures()),\n",
    "    (\"scaling\", StandardScaler()),\n",
    "    (\"selection_2\", SelectKBest(score_func=f_classif, k=700)),\n",
    "    (\"model\", eclf1)\n",
    "])\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "reshaper = FunctionTransformer(lambda X_: np.rollaxis(X_, 1).reshape(-1, 8)[:, 1::2], validate=False)\n",
    "stacking = Pipeline([\n",
    "    ('first_stage', eclf1),\n",
    "    ('reshaper', reshaper),\n",
    "    ('second_stage', LogisticRegression(C=100))\n",
    "])\n",
    "\n",
    "stacking.fit(x_train, y_train)\n",
    "train_score = stacking.score(x_train, y_train)\n",
    "test_score = stacking.score(x_test, y_test)\n",
    "test_preds = stacking.predict_proba(x_test)\n",
    "print(train_score, test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = stacking.fit(data_dummies, subscribed).predict_proba(holdout_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds_subscribed = pd.DataFrame(preds[:,1], columns=['subscribed'])\n",
    "submission = pd.concat([holdout_ids, preds_subscribed], axis=1)\n",
    "submission.to_csv(path_or_buf='preds.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
