{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loans Prediction - Predicting Successful Loan Subscriptions\n",
    "\n",
    "### Ian Johnson and Daniel First\n",
    "\n",
    "A banking institution ran a direct marketing campaign based on phone calls. Often, more than one contact to the same client was required, in order to assess if the product (bank term deposit) would be **subscribed** or **not**. Your task is to predict whether someone will subscribe to the term deposit or not based on the given information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0 - Import Libraries, Load Data\n",
    "\n",
    "This is the basic step where you can load the data and create train and test sets for internal validation as per your convinience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "# Features\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, Imputer, MaxAbsScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, SelectPercentile, VarianceThreshold\n",
    "from sklearn.feature_selection import RFE, f_classif, mutual_info_classif\n",
    "\n",
    "# Models - Linear\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, LassoCV, RidgeCV, ElasticNet, ElasticNetCV\n",
    "from sklearn.discriminant_analysis import  LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "# Models - Non-Linear\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Testing\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "pd.options.display.max_columns = 999\n",
    "delim = '\\n\\n' + '*'*30\n",
    "\n",
    "holdout = pd.read_csv('data/holdout.csv')\n",
    "data = pd.read_csv('data/data.csv')\n",
    "\n",
    "# Map labels to boolean values\n",
    "data['subscribed'] = data['subscribed'].map(lambda x: 0 if x == 'no' else 1)\n",
    "\n",
    "# Drop credit_defult as no one in holdout set has 'yes' value\n",
    "\n",
    "# ********\n",
    "subscribed = data['subscribed']\n",
    "data_withtarget=data\n",
    "# ***********\n",
    "\n",
    "holdout_ids = holdout['ID']\n",
    "data = data.drop([\"subscribed\", \"duration\", \"credit_default\"], axis=1)\n",
    "holdout = holdout.drop([\"ID\", \"duration\", \"credit_default\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>education</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>campaign</th>\n",
       "      <th>prev_days</th>\n",
       "      <th>prev_contacts</th>\n",
       "      <th>prev_outcomes</th>\n",
       "      <th>emp_var_rate</th>\n",
       "      <th>cons_price_idx</th>\n",
       "      <th>cons_conf_idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr_employed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.0</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.9y</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>apr</td>\n",
       "      <td>mon</td>\n",
       "      <td>2.0</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.695118</td>\n",
       "      <td>92.698705</td>\n",
       "      <td>-46.727552</td>\n",
       "      <td>1.345160</td>\n",
       "      <td>5097.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.0</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>may</td>\n",
       "      <td>wed</td>\n",
       "      <td>2.0</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.767159</td>\n",
       "      <td>92.914878</td>\n",
       "      <td>-46.313088</td>\n",
       "      <td>1.314499</td>\n",
       "      <td>5100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56.0</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.9y</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>cellular</td>\n",
       "      <td>nov</td>\n",
       "      <td>fri</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-0.100365</td>\n",
       "      <td>93.423076</td>\n",
       "      <td>-41.904559</td>\n",
       "      <td>4.003471</td>\n",
       "      <td>5193.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.0</td>\n",
       "      <td>retired</td>\n",
       "      <td>divorced</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>may</td>\n",
       "      <td>wed</td>\n",
       "      <td>4.0</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.771314</td>\n",
       "      <td>93.672814</td>\n",
       "      <td>-46.045500</td>\n",
       "      <td>1.261668</td>\n",
       "      <td>5100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34.0</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>jul</td>\n",
       "      <td>thu</td>\n",
       "      <td>8.0</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.458103</td>\n",
       "      <td>94.296285</td>\n",
       "      <td>-42.455877</td>\n",
       "      <td>5.152077</td>\n",
       "      <td>5233.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age           job marital_status          education housing loan  \\\n",
       "0  41.0   blue-collar        married           basic.9y     yes   no   \n",
       "1  46.0  entrepreneur        married            unknown      no   no   \n",
       "2  56.0    unemployed        married           basic.9y     yes  yes   \n",
       "3  89.0       retired       divorced           basic.4y     yes   no   \n",
       "4  34.0  entrepreneur        married  university.degree     yes   no   \n",
       "\n",
       "    contact month day_of_week  campaign  prev_days  prev_contacts  \\\n",
       "0  cellular   apr         mon       2.0        999              0   \n",
       "1  cellular   may         wed       2.0        999              0   \n",
       "2  cellular   nov         fri       1.0        999              0   \n",
       "3  cellular   may         wed       4.0        999              0   \n",
       "4  cellular   jul         thu       8.0        999              0   \n",
       "\n",
       "  prev_outcomes  emp_var_rate  cons_price_idx  cons_conf_idx  euribor3m  \\\n",
       "0   nonexistent     -1.695118       92.698705     -46.727552   1.345160   \n",
       "1   nonexistent     -1.767159       92.914878     -46.313088   1.314499   \n",
       "2   nonexistent     -0.100365       93.423076     -41.904559   4.003471   \n",
       "3   nonexistent     -1.771314       93.672814     -46.045500   1.261668   \n",
       "4   nonexistent      1.458103       94.296285     -42.455877   5.152077   \n",
       "\n",
       "   nr_employed  \n",
       "0       5097.0  \n",
       "1       5100.0  \n",
       "2       5193.0  \n",
       "3       5100.0  \n",
       "4       5233.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Exploration and Preparation\n",
    "\n",
    "In this step, we expect you to look into the data and try to understand it before modeling. This understanding may lead to some basic data preparation steps which are common across the two model sets required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categorical_df=data.select_dtypes(include=['object'])\n",
    "categorical_variables=categorical_df.columns\n",
    "\n",
    "frames = [categorical_df, subscribed]\n",
    "categ_and_target = pd.concat(frames,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we make a pairplot of all the features, and their relationships to one another:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pairplot1.png\" width=\"100%\" height=\"100%\" align=\"left\" />\n",
    "\n",
    "<img src=\"pairplot2.png\" width=\"100%\" height=\"100%\" align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot, we can observe some of the variables that may provide helpful splits between the successful and unsuccessful subscription attempts:\n",
    "\n",
    "- **cons_price_idx** - Consumer price index - monthly indicator (numeric)  \n",
    "\n",
    "- **age** - Age of the customer\n",
    "\n",
    "- **campaign** - The number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "\n",
    "- **duration** - This feature represents the duration of the current call for attempting to get the customer to subscribe. Because it proved to be such a good predictor of the final success rate, but it is not a variable that can be used to predict for future potential customers, we removed this feature in order to create a more useful and generalizable model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating new features\n",
    "#### Boolean variable indicating whether participant falls into subcategories that exhibit higher proportion of successful loan subscriptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jobs:\n",
      "{'management': 0, 'retired': 0, 'self-employed': 0, 'unknown': 0, 'unemployed': 0, 'housemaid': 0, 'admin.': 0, 'entrepreneur': 0, 'services': 0, 'student': 0, 'technician': 0, 'blue-collar': 0}\n",
      "Months:\n",
      "{'mar': 0, 'aug': 0, 'sep': 0, 'apr': 0, 'jun': 0, 'jul': 0, 'may': 0, 'nov': 0, 'dec': 0, 'oct': 0}\n",
      "Education:\n",
      "{'basic.9y': 0, 'illiterate': 0, 'basic.4y': 0, 'unknown': 0, 'basic.6y': 0, 'high.school': 0, 'professional.course': 0, 'university.degree': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Jobs:\")\n",
    "dict_percentage_job={}\n",
    "job_categories=categ_and_target.job.unique()\n",
    "\n",
    "for each_job in job_categories:\n",
    "    just_that_job_df=categ_and_target[categ_and_target.job==each_job]\n",
    "    percentage=len(just_that_job_df[just_that_job_df.subscribed==1])/len(just_that_job_df)\n",
    "    dict_percentage_job[each_job]=percentage\n",
    "print(dict_percentage_job)\n",
    "\n",
    "print(\"Months:\")\n",
    "dict_percentage_month={}\n",
    "month_categories=categ_and_target.month.unique()\n",
    "\n",
    "for each_month in month_categories:\n",
    "    just_that_month_df=categ_and_target[categ_and_target.month==each_month]\n",
    "    percentage=len(just_that_month_df[just_that_month_df.subscribed==1])/len(just_that_month_df)\n",
    "    dict_percentage_month[each_month]=percentage\n",
    "print(dict_percentage_month)\n",
    "\n",
    "\n",
    "print(\"Education:\")\n",
    "dict_percentage_edu={}\n",
    "edu_categories=categ_and_target.education.unique()\n",
    "\n",
    "for each_edu in edu_categories:\n",
    "    just_that_edu_df=categ_and_target[categ_and_target.education==each_edu]\n",
    "    percentage=len(just_that_edu_df[just_that_edu_df.subscribed==1])/len(just_that_edu_df)\n",
    "    dict_percentage_edu[each_edu]=percentage\n",
    "print(dict_percentage_edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def month_function(month):\n",
    "    if month==\"dec\" or month==\"mar\" or month==\"oct\" or month==\"sep\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def job_function(job):\n",
    "    if job==\"student\" or job == \"retired\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Note: 0s and 1s switched\n",
    "def education_function(y):\n",
    "    if y==\"basic.9y\" or y==\"basic.4y\" or y==\"basic.6y\" or y==\"high.school\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_counts(x,dict):\n",
    "    return dict[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def new_feats(data):\n",
    "    data_withbool=data\n",
    "    \n",
    "    data_withbool[\"monthbool\"]=data.month.apply(lambda x:month_function(x))\n",
    "    data_withbool[\"jobbool\"]=data.job.apply(lambda x:job_function(x))\n",
    "    data_withbool[\"educationbool\"]=data.education.apply(lambda x:education_function(x))\n",
    "    \n",
    "    data_withbool_withcounts=data_withbool\n",
    "    data_withbool_withcounts[\"educationcounts\"]=data.education.apply(lambda x:get_counts(x,dict_percentage_edu))\n",
    "    data_withbool_withcounts[\"jobcounts\"]=data.job.apply(lambda x:get_counts(x,dict_percentage_job))\n",
    "    data_withbool_withcounts[\"monthcounts\"]=data.month.apply(lambda x:get_counts(x,dict_percentage_month))\n",
    "    \n",
    "    dict_edu_2={}\n",
    "    dict_edu_2[\"basic.4y\"]=1\n",
    "    dict_edu_2[\"basic.6y\"]=2\n",
    "    dict_edu_2[\"basic.9y\"]=3\n",
    "    dict_edu_2[\"high.school\"]=4\n",
    "    dict_edu_2[\"professional.course\"]=5\n",
    "    dict_edu_2[\"university.degree\"]=6\n",
    "    dict_edu_2[\"unknown\"]=7\n",
    "    dict_edu_2[\"illiterate\"]=8\n",
    "\n",
    "    data_withbool_withcounts[\"edu_linear\"]=data.education.apply(lambda x:get_counts(x,dict_edu_2))\n",
    "    \n",
    "    return data_withbool_withcounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features consisting of the logs of each of the continuous columns from the datasest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Log features\n",
    "def log_feats(data):\n",
    "    for c in data.select_dtypes(exclude=['object']).columns:\n",
    "        data[c + '__log'] = data[c].apply(lambda x: np.log(abs(x) + 0.001))\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features consisting of the mean of each of the continuous columns, for each subcategory of the categorical columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_mean_cont_per_cat_group(df):\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    continuous_cols = df.select_dtypes(exclude=['object']).columns\n",
    "    \n",
    "    means = {}\n",
    "    \n",
    "    for cat in categorical_cols:\n",
    "        for u in df[cat].unique():\n",
    "            for con in continuous_cols:\n",
    "                subset = df[cat] == u\n",
    "                mean_value = df.loc[subset, con].mean()\n",
    "                \n",
    "                if not cat in means:\n",
    "                    means[cat] = {}\n",
    "                else:\n",
    "                    means[cat][u] = mean_value\n",
    "                \n",
    "    return means\n",
    "\n",
    "\n",
    "def transform_mean_cont_per_cat_group(df, means):\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    continuous_cols = df.select_dtypes(exclude=['object']).columns\n",
    "    \n",
    "    for cat in categorical_cols:\n",
    "        for u in df[cat].unique():\n",
    "            for con in continuous_cols:\n",
    "                \n",
    "                subset = df[cat] == u\n",
    "                mean_value = means[cat][u]\n",
    "                df.loc[subset, cat + '__' + con + '__mean'] = mean_value\n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply new feature creation functions to the data and holdout data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32950, 18)\n",
      "(8238, 18)\n",
      "(32950, 99)\n",
      "(8238, 99)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(holdout.shape)\n",
    "means = fit_mean_cont_per_cat_group(data)\n",
    "data_new_feats_1 = transform_mean_cont_per_cat_group(data, means)\n",
    "holdout_new_feats_1 = transform_mean_cont_per_cat_group(holdout, means)\n",
    "print(data_new_feats_1.shape)\n",
    "print(holdout_new_feats_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32950, 106)\n",
      "(8238, 106)\n"
     ]
    }
   ],
   "source": [
    "data_new_feats_2 = new_feats(data)\n",
    "holdout_new_feats_2 = new_feats(holdout)\n",
    "print(data_new_feats_2.shape)\n",
    "print(holdout_new_feats_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32950, 203)\n",
      "(8238, 203)\n"
     ]
    }
   ],
   "source": [
    "data_new_feats_3 = log_feats(data_new_feats_2)\n",
    "holdout_new_feats_3 = log_feats(holdout_new_feats_2)\n",
    "print(data_new_feats_3.shape)\n",
    "print(holdout_new_feats_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32950, 244)\n",
      "(8238, 244)\n"
     ]
    }
   ],
   "source": [
    "data_dummies = pd.get_dummies(data_new_feats_3)\n",
    "holdout_dummies = pd.get_dummies(holdout_new_feats_3)\n",
    "print(data_dummies.shape)\n",
    "print(holdout_dummies.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split of our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24712, 244)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data_dummies, subscribed, random_state=42)# stratify=subscribed, random_state=42)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility grid search function to assess subsequent models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grid_search_metrics(pipe, param_grid):\n",
    "    grid = GridSearchCV(pipe, param_grid=param_grid, scoring='roc_auc')\n",
    "    grid.fit(x_train, y_train)\n",
    "    print(\"Best GS score: {}\".format(grid.best_score_))\n",
    "    print(\"Best params: {}\".format(grid.best_params_))\n",
    "    score = grid.score(x_test, y_test)\n",
    "    print(\"Best test score: {}\".format(score))\n",
    "    print(\"Overfitting amount: {}\".format(grid.best_score_ - score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Model Set 1\n",
    "\n",
    "In this step, we perform the following steps relevant to exploring our initial options for modeling:\n",
    "\n",
    "* **validation**\n",
    "* **feature selection**\n",
    "* **final model selection**\n",
    "\n",
    "We limit ourselves to linear models for now.\n",
    "\n",
    "You will notice that we chose a pipline with the following steps (formed from previous experiementation not shown in this notebook):\n",
    "\n",
    "- **Variance Thresholding:** To remove constant features across the dataset (discovered from multiple warnings during classifier training)\n",
    "\n",
    "- **Select K Best:** Our initial regularization step, to prevent slowdown, in addition to too many features coming from Polynomial Feature creation\n",
    "\n",
    "- **Polynomial Features:** To explore feature interactions + higher-order relationships between our variables and the `subscribed` column\n",
    "\n",
    "- **Scaling:** To give features equal importance in non-tree based classification methods\n",
    "    - Logistic regression, SVMs, perceptrons, neural networks will have their weights updated in inconsistent amounts across the features if the scales of the features are not identical\n",
    "    - Linear discriminant analysis will preferentially weight the features whose scale is larger, as it attempts to compute the features that form the direction of maximal variance\n",
    "    \n",
    "- **Second Select K Best:** After feature interactions, and scaling, we would like to re-select the new top features as another form of regularization\n",
    "\n",
    "- **Model:** The classification model (either a single model, Voting Classifier, or Stacked Ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - Best score: 0.7608"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best GS score: 0.759512868405\n",
      "Best params: {'model__C': 0.5}\n",
      "Best test score: 0.760780878347\n",
      "Overfitting amount: -0.00126800994195\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"variance\", VarianceThreshold()),\n",
    "    (\"selection_1\", SelectKBest(score_func=f_classif)),\n",
    "    (\"polys\", PolynomialFeatures()),\n",
    "    (\"scaling\", MinMaxScaler()),\n",
    "    (\"selection_2\", SelectKBest(score_func=f_classif)),\n",
    "    (\"model\", LogisticRegression())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'model__C': [.5], #[1, 0.5, .1, .001]\n",
    "}\n",
    "\n",
    "grid_search_metrics(pipe, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Support Vector Machine - Best score: 0.7601"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best GS score: 0.760879465153\n",
      "Best params: {'model__C': 0.3}\n",
      "Best test score: 0.760114330744\n",
      "Overfitting amount: 0.000765134408047\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"variance\", VarianceThreshold()),\n",
    "    (\"selection_1\", SelectKBest(score_func=f_classif)),\n",
    "    (\"polys\", PolynomialFeatures()),\n",
    "    (\"scaling\", MinMaxScaler()),\n",
    "    (\"selection_2\", SelectKBest(score_func=f_classif)),\n",
    "    (\"model\", LinearSVC())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'model__C': [0.3, 0.1, 0.05], #[1, .5, .1, .001]\n",
    "}\n",
    "\n",
    "grid_search_metrics(pipe, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression - Best score: 0.7601"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best GS score: 0.760884851594\n",
      "Best params: {'model__alpha': 1}\n",
      "Best test score: 0.76009592023\n",
      "Overfitting amount: 0.000788931363888\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"variance\", VarianceThreshold()),\n",
    "    (\"selection_1\", SelectKBest(score_func=f_classif)),\n",
    "    (\"polys\", PolynomialFeatures()),\n",
    "    (\"scaling\", MinMaxScaler()),\n",
    "    (\"selection_2\", SelectKBest(score_func=f_classif)),\n",
    "    (\"model\", RidgeClassifier())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'model__alpha': [1, 5, 10, 100]\n",
    "}\n",
    "\n",
    "grid_search_metrics(pipe, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis  - Best score: 0.7601"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best GS score: 0.773110895248\n",
      "Best params: {'model__store_covariance': False}\n",
      "Best test score: 0.7600980947\n",
      "Overfitting amount: 0.0130128005481\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"variance\", VarianceThreshold()),\n",
    "    (\"selection_1\", SelectKBest(score_func=f_classif)),\n",
    "    (\"polys\", PolynomialFeatures()),\n",
    "    (\"scaling\", MinMaxScaler()),\n",
    "    (\"selection_2\", SelectKBest(score_func=f_classif)),\n",
    "    (\"model\", LinearDiscriminantAnalysis())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'model__store_covariance': [False]\n",
    "}\n",
    "\n",
    "grid_search_metrics(pipe, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes - Best score: 0.7558"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best GS score: 0.755946121809\n",
      "Best params: {'model__priors': [0.88, 0.12]}\n",
      "Best test score: 0.755813591077\n",
      "Overfitting amount: 0.000132530732001\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"variance\", VarianceThreshold()),\n",
    "    (\"selection_1\", SelectKBest(score_func=f_classif)),\n",
    "    (\"polys\", PolynomialFeatures()),\n",
    "    (\"scaling\", MinMaxScaler()),\n",
    "    (\"selection_2\", SelectKBest(score_func=f_classif)),\n",
    "    (\"model\", GaussianNB())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'model__priors': [[0.88, 0.12]]\n",
    "}\n",
    "\n",
    "grid_search_metrics(pipe, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Model Set 2\n",
    "\n",
    "\n",
    "In this step, we perform the following steps relevant to exploring our initial options for modeling:\n",
    "\n",
    "* validation\n",
    "* feature selection\n",
    "* final model selection\n",
    "\n",
    "We explore non-linear methods in this model set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier - Best score: 0.7828"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best GS score: 0.788095295181\n",
      "Best params: {'model__max_features': 14, 'selection_1__k': 55, 'model__criterion': 'entropy', 'model__max_depth': 3, 'selection_2__k': 600, 'model__min_samples_leaf': 10, 'model__min_samples_split': 2, 'model__bootstrap': True}\n",
      "Best test score: 0.782781588094\n",
      "Overfitting amount: 0.00531370708682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [   0  163 1594] are constant.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"variance\", VarianceThreshold()),\n",
    "    (\"selection_1\", SelectKBest(score_func=f_classif)),\n",
    "    (\"polys\", PolynomialFeatures()),\n",
    "    (\"scaling\", StandardScaler()),\n",
    "    (\"selection_2\", SelectKBest(score_func=f_classif)),\n",
    "    (\"model\", RandomForestClassifier())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'selection_1__k': [55], #[45, 50, 55],\n",
    "    'selection_2__k': [600], #[600, 700, 800],\n",
    "    \n",
    "    \"model__max_depth\": [3], #[3, None],\n",
    "    \"model__max_features\": [14], #[13, 14, 15],\n",
    "    \"model__min_samples_split\": [2], #[2, 3, 4],\n",
    "    \"model__min_samples_leaf\": [10], #[7, 10, 15],\n",
    "    \"model__bootstrap\": [True], #[True, False],\n",
    "    \"model__criterion\": ['entropy'], #[\"gini\", \"entropy\"]\n",
    "}\n",
    "\n",
    "grid_search_metrics(pipe, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier - Best score: 0.7846"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best GS score: 0.787467734305\n",
      "Best params: {'model__max_features': 'sqrt', 'selection_1__k': 50, 'model__learning_rate': 0.1, 'model__max_depth': 6, 'selection_2__k': 700, 'model__min_samples_leaf': 40, 'model__min_samples_split': 600, 'model__random_state': 10, 'model__subsample': 0.8}\n",
      "Best test score: 0.784558637634\n",
      "Overfitting amount: 0.0029090966718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [   0  148 1324] are constant.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"variance\", VarianceThreshold()),\n",
    "    (\"selection_1\", SelectKBest(score_func=f_classif)),\n",
    "    (\"polys\", PolynomialFeatures()),\n",
    "    (\"scaling\", StandardScaler()),\n",
    "    (\"selection_2\", SelectKBest(score_func=f_classif)),\n",
    "    (\"model\", GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'selection_1__k': [50],\n",
    "    'selection_2__k': [700],\n",
    "    \n",
    "    'model__max_features': ['sqrt'],\n",
    "    'model__learning_rate': [.1],\n",
    "    'model__max_depth': [6], # [5, 6, 7],\n",
    "    'model__min_samples_leaf': [40], # [39, 40, 41],\n",
    "    'model__min_samples_split': [600], #[590, 600, 610],\n",
    "    'model__random_state': [10],\n",
    "    'model__subsample': [0.8], #[0.75, 0.8, 0.85],\n",
    "}\n",
    "\n",
    "grid_search_metrics(pipe, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost - Best score: 0.7801"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best GS score: 0.785944465866\n",
      "Best params: {'selection_1__k': 50, 'model__colsample_bytree': 1.0, 'model__max_depth': 5, 'selection_2__k': 700, 'model__subsample': 0.9}\n",
      "Best test score: 0.780086187301\n",
      "Overfitting amount: 0.00585827856498\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"variance\", VarianceThreshold()),\n",
    "    (\"selection_1\", SelectKBest(score_func=f_classif)),\n",
    "    (\"polys\", PolynomialFeatures()),\n",
    "    (\"scaling\", StandardScaler()),\n",
    "    (\"selection_2\", SelectKBest(score_func=f_classif)),\n",
    "    (\"model\", XGBClassifier())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'selection_1__k': [50],\n",
    "    'selection_2__k': [700],\n",
    "    \n",
    "#     'model__num_boost_round': [100], #[100, 250, 500],\n",
    "#     'model__eta': [0.05], #[0.05, 0.1, 0.3],\n",
    "    'model__max_depth': [5], #, 6, 9],\n",
    "    'model__subsample': [0.9], #, 1.0],\n",
    "    'model__colsample_bytree': [1.0] #, [0.9, 1.0],\n",
    "}\n",
    "\n",
    "grid_search_metrics(pipe, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best GS score: 0.605054604871\n",
      "Best params: {'selection_2__k': 700, 'model__learning_rate': 1.1, 'model__base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'), 'model__n_estimators': 100, 'selection_1__k': 50}\n",
      "Best test score: 0.614430045264\n",
      "Overfitting amount: -0.00937544039299\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"variance\", VarianceThreshold()),\n",
    "    (\"selection_1\", SelectKBest(score_func=f_classif)),\n",
    "    (\"polys\", PolynomialFeatures()),\n",
    "    (\"scaling\", StandardScaler()),\n",
    "    (\"selection_2\", SelectKBest(score_func=f_classif)),\n",
    "    (\"model\", AdaBoostClassifier())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'selection_1__k': [50],\n",
    "    'selection_2__k': [700],\n",
    "    \n",
    "    'model__base_estimator': [DecisionTreeClassifier()],\n",
    "    'model__n_estimators': [100, 110], #[50, 100],\n",
    "    'model__learning_rate': [1.0, 1.1], #[0.5, 1.0]\n",
    "}\n",
    "\n",
    "grid_search_metrics(pipe, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron Classifier - Best score: 0.7873"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best GS score: 0.776304905481\n",
      "Best params: {'model__hidden_layer_sizes': (10, 10, 10), 'model__alpha': 0.0001, 'selection_2__k': 700, 'selection_1__k': 50}\n",
      "Best test score: 0.787252008921\n",
      "Overfitting amount: -0.0109471034393\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"variance\", VarianceThreshold()),\n",
    "    (\"selection_1\", SelectKBest(score_func=f_classif)),\n",
    "    (\"polys\", PolynomialFeatures()),\n",
    "    (\"scaling\", StandardScaler()),\n",
    "    (\"selection_2\", SelectKBest(score_func=f_classif)),\n",
    "    (\"model\", MLPClassifier())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'selection_1__k': [50],\n",
    "    'selection_2__k': [700],\n",
    "    \n",
    "    'model__alpha': [0.0001], #, 0.001],\n",
    "    'model__hidden_layer_sizes': [(10, 10, 10)], #[(5, 5, 5), (10, 10, 10), (20, 20, 20), (30, 30, 30), (40, 40, 40)]\n",
    "}\n",
    "\n",
    "grid_search_metrics(pipe, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Ensemble\n",
    "\n",
    "In this step, we ensemble the best of our tuned models from the previous steps:\n",
    "- LogisticRegression\n",
    "- RandomForest\n",
    "- GradientBoosting\n",
    "- MultiLayerPerceptron\n",
    "\n",
    "Our final choice for this section, after multiple runs with different pipeline parameters, achieved a test score of:\n",
    "\n",
    "### 0.7898\n",
    "\n",
    "This is an improvement of about\n",
    "\n",
    "### 0.002\n",
    "\n",
    "over our individual tuned models on their own. We will later explore stacking with this ensemble model to make further improvements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# svc = LinearSVC(C=0.5)\n",
    "# clf1 = CalibratedClassifierCV(svc, method='sigmoid')\n",
    "\n",
    "lr = LogisticRegression(C=0.5)\n",
    "clf1 = CalibratedClassifierCV(lr, method='sigmoid')\n",
    "\n",
    "rf1 = RandomForestClassifier(\n",
    "    max_features = 10,\n",
    "    criterion = 'gini',\n",
    "    max_depth = 3,\n",
    "    min_samples_leaf = 10,\n",
    "    min_samples_split = 3,\n",
    "    bootstrap = True,\n",
    ")\n",
    "clf2 = CalibratedClassifierCV(rf1, method='sigmoid')\n",
    "\n",
    "gb1 = GradientBoostingClassifier(\n",
    "    max_features='sqrt',\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    min_samples_leaf=40,\n",
    "    min_samples_split=600,\n",
    "    random_state=10,\n",
    "    subsample=0.8\n",
    ")\n",
    "clf3 = CalibratedClassifierCV(gb1, method='sigmoid')\n",
    "\n",
    "clf4 = MLPClassifier(alpha = 0.0001, hidden_layer_sizes = (20, 20, 20))\n",
    "\n",
    "\n",
    "eclf1 = VotingClassifier(voting='soft', estimators=[\n",
    "    ('one', clf1),\n",
    "    ('two', clf2),\n",
    "    ('three', clf3),\n",
    "    ('four', clf4),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"variance\", VarianceThreshold()),\n",
    "    (\"selection_1\", SelectKBest(score_func=f_classif)),\n",
    "    (\"polys\", PolynomialFeatures()),\n",
    "    (\"scaling\", StandardScaler()),\n",
    "    (\"selection_2\", SelectKBest(score_func=f_classif)),\n",
    "    (\"model\", eclf1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best GS score: 0.794581376016\n",
      "Best params: {'model__voting': 'soft', 'selection_2__k': 900, 'selection_1__k': 70}\n",
      "Best test score: 0.78979773368\n",
      "Overfitting amount: 0.0047836423357\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'selection_1__k': [70],\n",
    "    'selection_2__k': [900],\n",
    "    'model__voting': ['soft'],\n",
    "}\n",
    "\n",
    "grid_search_metrics(pipe, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking\n",
    "\n",
    "Here we explore the use of [stacking](https://en.wikipedia.org/wiki/Ensemble_learning#Stacking) to improve our test score. \n",
    "\n",
    "We apply a Logistic Regression Classifier on top of the predicted probabilities from the previous Voting Classifier ensemble to achieve an AUC_ROC score of:\n",
    "\n",
    "### 0.7930\n",
    "\n",
    "Which is an improvement of \n",
    "\n",
    "### 0.0032\n",
    "\n",
    "over the previous ensemble alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"variance\", VarianceThreshold()),\n",
    "    (\"selection_1\", SelectKBest(score_func=f_classif, k=50)),\n",
    "    (\"polys\", PolynomialFeatures()),\n",
    "    (\"scaling\", StandardScaler()),\n",
    "    (\"selection_2\", SelectKBest(score_func=f_classif, k=700)),\n",
    "    (\"model\", eclf1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.90931531239883456, 0.89621267297887841)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "reshaper = FunctionTransformer(lambda X_: np.rollaxis(X_, 1).reshape(-1, 8)[:, 1::2], validate=False)\n",
    "stacking = Pipeline([\n",
    "    ('first_stage', eclf1),\n",
    "    ('reshaper', reshaper),\n",
    "    ('second_stage', LogisticRegression(C=100))\n",
    "])\n",
    "\n",
    "stacking.fit(x_train, y_train)\n",
    "train_score = stacking.score(x_train, y_train)\n",
    "test_score = stacking.score(x_test, y_test)\n",
    "test_preds = stacking.predict_proba(x_test)\n",
    "print(train_score, test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79299000091617677"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_score = aoc_auc_score(y_test, test_preds[:,1])\n",
    "assert final_score > 0.79"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holdout Predictions with best model\n",
    "\n",
    "Finally we make predictions on the holdout set with our best stacked ensemble for submission to the Kaggle competition. Our final score on the private leaderboard was:\n",
    "\n",
    "### 0.7881\n",
    "\n",
    "After a total of **14** submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.90887018452573654, 0.89633406166545282)\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"variance\", VarianceThreshold()),\n",
    "    (\"selection_1\", SelectKBest(score_func=f_classif, k=50)),\n",
    "    (\"polys\", PolynomialFeatures()),\n",
    "    (\"scaling\", StandardScaler()),\n",
    "    (\"selection_2\", SelectKBest(score_func=f_classif, k=700)),\n",
    "    (\"model\", eclf1)\n",
    "])\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "reshaper = FunctionTransformer(lambda X_: np.rollaxis(X_, 1).reshape(-1, 8)[:, 1::2], validate=False)\n",
    "stacking = Pipeline([\n",
    "    ('first_stage', eclf1),\n",
    "    ('reshaper', reshaper),\n",
    "    ('second_stage', LogisticRegression(C=100))\n",
    "])\n",
    "\n",
    "stacking.fit(x_train, y_train)\n",
    "train_score = stacking.score(x_train, y_train)\n",
    "test_score = stacking.score(x_test, y_test)\n",
    "test_preds = stacking.predict_proba(x_test)\n",
    "print(train_score, test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = stacking.fit(data_dummies, subscribed).predict_proba(holdout_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds_subscribed = pd.DataFrame(preds[:,1], columns=['subscribed'])\n",
    "submission = pd.concat([holdout_ids, preds_subscribed], axis=1)\n",
    "submission.to_csv(path_or_buf='preds.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Results + Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the course of the competition, we were able to climb to our best position of **7th** place on the public leaderboard with the final ensemble shown above. These were encouraging results, however our position dropped significantly when the private scores were opened, and our final position was **28th** out of **66** teams.\n",
    "\n",
    "This reason for this drop is the small margin between tuned classifiers and their benchmark counterparts. A difference of .001 was significant in terms of leaderboard positions, and thus they were particularly vulnerable to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
